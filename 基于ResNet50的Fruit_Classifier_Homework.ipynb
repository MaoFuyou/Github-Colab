{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOU/+nqUkafPbE56rcULA4C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maoeyu/Github-Colab/blob/main/%E5%9F%BA%E4%BA%8EResNet50%E7%9A%84Fruit_Classifier_Homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "题目：基于 ResNet50 的水果分类\n",
        "\n",
        "背景：使用基于卷积的深度神经网络 ResNet50 对 30 种水果进行分类\n",
        "\n",
        "任务\n",
        "\n",
        "划分训练集和验证集\n",
        "按照 MMPreTrain CustomDataset 格式组织训练集和验证集\n",
        "使用 MMPreTrain 算法库，编写配置文件，正确加载预训练模型\n",
        "在水果数据集上进行微调训练\n",
        "使用 MMPreTrain 的 ImageClassificationInferencer 接口，对网络水果图像，或自己拍摄的水果图像，使用训练好的模型进行分类\n",
        "需提交的验证集评估指标（不能低于 60%）\n",
        "\n",
        "ResNet-50\n",
        "\n",
        "\n",
        "作业数据集下载： 链接：https://pan.baidu.com/s/1YgoU1M_v7ridtXB9xxbA1Q 提取码：52m9\n",
        "\n",
        "课程中猫狗数据集下载地址： https://download.openmmlab.com/mmclassification/dataset/cats_dogs_dataset.tar"
      ],
      "metadata": {
        "id": "1TsRqTky2UbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 删掉原有的 mmpretrain 文件夹（如有）\n",
        "!rm -rf mmpretrain"
      ],
      "metadata": {
        "id": "mSJrJY2o6GzF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.MMPreTrain**"
      ],
      "metadata": {
        "id": "TO54XZNJYXSf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shj1iw8D2L7B",
        "outputId": "40fcf70a-f571-4c0f-d27c-b07916a6333e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mmpretrain'...\n",
            "remote: Enumerating objects: 16733, done.\u001b[K\n",
            "remote: Counting objects: 100% (1564/1564), done.\u001b[K\n",
            "remote: Compressing objects: 100% (675/675), done.\u001b[K\n",
            "remote: Total 16733 (delta 986), reused 1313 (delta 870), pack-reused 15169\u001b[K\n",
            "Receiving objects: 100% (16733/16733), 13.27 MiB | 25.12 MiB/s, done.\n",
            "Resolving deltas: 100% (11607/11607), done.\n",
            "/content/mmpretrain\n",
            "Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/mmpretrain\n",
            "  Running command python setup.py egg_info\n",
            "  running egg_info\n",
            "  creating /tmp/pip-pip-egg-info-7sg5azac/mmpretrain.egg-info\n",
            "  writing /tmp/pip-pip-egg-info-7sg5azac/mmpretrain.egg-info/PKG-INFO\n",
            "  writing dependency_links to /tmp/pip-pip-egg-info-7sg5azac/mmpretrain.egg-info/dependency_links.txt\n",
            "  writing requirements to /tmp/pip-pip-egg-info-7sg5azac/mmpretrain.egg-info/requires.txt\n",
            "  writing top-level names to /tmp/pip-pip-egg-info-7sg5azac/mmpretrain.egg-info/top_level.txt\n",
            "  writing manifest file '/tmp/pip-pip-egg-info-7sg5azac/mmpretrain.egg-info/SOURCES.txt'\n",
            "  reading manifest file '/tmp/pip-pip-egg-info-7sg5azac/mmpretrain.egg-info/SOURCES.txt'\n",
            "  reading manifest template 'MANIFEST.in'\n",
            "  warning: no files found matching 'mmpretrain/.mim/model-index.yml'\n",
            "  warning: no files found matching '*.py' under directory 'mmpretrain/.mim/configs'\n",
            "  warning: no files found matching '*.yml' under directory 'mmpretrain/.mim/configs'\n",
            "  warning: no files found matching '*.py' under directory 'mmpretrain/.mim/tools'\n",
            "  warning: no files found matching '*.sh' under directory 'mmpretrain/.mim/tools'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file '/tmp/pip-pip-egg-info-7sg5azac/mmpretrain.egg-info/SOURCES.txt'\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting einops (from mmpretrain==1.0.0rc8)\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib-metadata (from mmpretrain==1.0.0rc8)\n",
            "  Downloading importlib_metadata-6.6.0-py3-none-any.whl (22 kB)\n",
            "Collecting mat4py (from mmpretrain==1.0.0rc8)\n",
            "  Downloading mat4py-0.5.0-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mmpretrain==1.0.0rc8) (3.7.1)\n",
            "Collecting modelindex (from mmpretrain==1.0.0rc8)\n",
            "  Downloading modelindex-0.0.2-py3-none-any.whl (2.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mmpretrain==1.0.0rc8) (1.22.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from mmpretrain==1.0.0rc8) (13.3.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->mmpretrain==1.0.0rc8) (3.15.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmpretrain==1.0.0rc8) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmpretrain==1.0.0rc8) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmpretrain==1.0.0rc8) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmpretrain==1.0.0rc8) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmpretrain==1.0.0rc8) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmpretrain==1.0.0rc8) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmpretrain==1.0.0rc8) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmpretrain==1.0.0rc8) (2.8.2)\n",
            "Collecting model-index (from modelindex->mmpretrain==1.0.0rc8)\n",
            "  Downloading model_index-0.1.11-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->mmpretrain==1.0.0rc8) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->mmpretrain==1.0.0rc8) (2.14.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->mmpretrain==1.0.0rc8) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mmpretrain==1.0.0rc8) (1.16.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from model-index->modelindex->mmpretrain==1.0.0rc8) (6.0)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from model-index->modelindex->mmpretrain==1.0.0rc8) (3.4.3)\n",
            "Collecting ordered-set (from model-index->modelindex->mmpretrain==1.0.0rc8)\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from model-index->modelindex->mmpretrain==1.0.0rc8) (8.1.3)\n",
            "Installing collected packages: mat4py, ordered-set, importlib-metadata, einops, model-index, modelindex, mmpretrain\n",
            "  changing mode of /usr/local/bin/mi to 755\n",
            "  Running setup.py develop for mmpretrain\n",
            "    Running command python setup.py develop\n",
            "    running develop\n",
            "    /usr/local/lib/python3.10/dist-packages/setuptools/command/develop.py:40: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "    !!\n",
            "\n",
            "            ********************************************************************************\n",
            "            Please avoid running ``setup.py`` and ``easy_install``.\n",
            "            Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "            other standards-based tools.\n",
            "\n",
            "            See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "            ********************************************************************************\n",
            "\n",
            "    !!\n",
            "      easy_install.initialize_options(self)\n",
            "    /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "    !!\n",
            "\n",
            "            ********************************************************************************\n",
            "            Please avoid running ``setup.py`` directly.\n",
            "            Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "            other standards-based tools.\n",
            "\n",
            "            See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "            ********************************************************************************\n",
            "\n",
            "    !!\n",
            "      self.initialize_options()\n",
            "    running egg_info\n",
            "    creating mmpretrain.egg-info\n",
            "    writing mmpretrain.egg-info/PKG-INFO\n",
            "    writing dependency_links to mmpretrain.egg-info/dependency_links.txt\n",
            "    writing requirements to mmpretrain.egg-info/requires.txt\n",
            "    writing top-level names to mmpretrain.egg-info/top_level.txt\n",
            "    writing manifest file 'mmpretrain.egg-info/SOURCES.txt'\n",
            "    reading manifest file 'mmpretrain.egg-info/SOURCES.txt'\n",
            "    reading manifest template 'MANIFEST.in'\n",
            "    adding license file 'LICENSE'\n",
            "    writing manifest file 'mmpretrain.egg-info/SOURCES.txt'\n",
            "    running build_ext\n",
            "    Creating /usr/local/lib/python3.10/dist-packages/mmpretrain.egg-link (link to .)\n",
            "    Adding mmpretrain 1.0.0rc8 to easy-install.pth file\n",
            "\n",
            "    Installed /content/mmpretrain\n",
            "Successfully installed einops-0.6.1 importlib-metadata-6.6.0 mat4py-0.5.0 mmpretrain-1.0.0rc8 model-index-0.1.11 modelindex-0.0.2 ordered-set-4.1.0\n"
          ]
        }
      ],
      "source": [
        "#事前工作 相关配置\n",
        "!git clone https://github.com/open-mmlab/mmpretrain.git\n",
        "%cd mmpretrain\n",
        "!pip install -v -e ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U openmim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raLjhLHrYjF0",
        "outputId": "578e3211-5ed4-4e25-917b-35d6a8c660e6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openmim\n",
            "  Downloading openmim-0.3.7-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click in /usr/local/lib/python3.10/dist-packages (from openmim) (8.1.3)\n",
            "Collecting colorama (from openmim)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: model-index in /usr/local/lib/python3.10/dist-packages (from openmim) (0.1.11)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from openmim) (1.5.3)\n",
            "Requirement already satisfied: pip>=19.3 in /usr/local/lib/python3.10/dist-packages (from openmim) (23.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from openmim) (2.27.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from openmim) (13.3.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from openmim) (0.8.10)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from model-index->openmim) (6.0)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from model-index->openmim) (3.4.3)\n",
            "Requirement already satisfied: ordered-set in /usr/local/lib/python3.10/dist-packages (from model-index->openmim) (4.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->openmim) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->openmim) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->openmim) (1.22.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (3.4)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->openmim) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->openmim) (2.14.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->openmim) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->openmim) (1.16.0)\n",
            "Installing collected packages: colorama, openmim\n",
            "Successfully installed colorama-0.4.6 openmim-0.3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mim install -e \".[multimodal]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jduOB9OjY87_",
        "outputId": "8f164472-7936-434a-a698-f838d27ee0c3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.openmmlab.com/mmcv/dist/cu118/torch2.0.0/index.html\n",
            "Obtaining file:///content/mmpretrain\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from mmpretrain==1.0.0rc8) (0.6.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from mmpretrain==1.0.0rc8) (6.6.0)\n",
            "Requirement already satisfied: mat4py in /usr/local/lib/python3.10/dist-packages (from mmpretrain==1.0.0rc8) (0.5.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mmpretrain==1.0.0rc8) (3.7.1)\n",
            "Requirement already satisfied: modelindex in /usr/local/lib/python3.10/dist-packages (from mmpretrain==1.0.0rc8) (0.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mmpretrain==1.0.0rc8) (1.22.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from mmpretrain==1.0.0rc8) (13.3.4)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from mmpretrain==1.0.0rc8) (2.0.6)\n",
            "Collecting transformers>=4.28.0 (from mmpretrain==1.0.0rc8)\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mmcv<2.1.0,>=2.0.0rc4 (from mmpretrain==1.0.0rc8)\n",
            "  Downloading https://download.openmmlab.com/mmcv/dist/cu118/torch2.0.0/mmcv-2.0.0-cp310-cp310-manylinux1_x86_64.whl (74.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.4/74.4 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mmengine<1.0.0,>=0.4.0 (from mmpretrain==1.0.0rc8)\n",
            "  Downloading mmengine-0.7.4-py3-none-any.whl (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.3/374.3 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting addict (from mmcv<2.1.0,>=2.0.0rc4->mmpretrain==1.0.0rc8)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mmcv<2.1.0,>=2.0.0rc4->mmpretrain==1.0.0rc8) (23.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from mmcv<2.1.0,>=2.0.0rc4->mmpretrain==1.0.0rc8) (8.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from mmcv<2.1.0,>=2.0.0rc4->mmpretrain==1.0.0rc8) (6.0)\n",
            "Collecting yapf (from mmcv<2.1.0,>=2.0.0rc4->mmpretrain==1.0.0rc8)\n",
            "  Downloading yapf-0.33.0-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.9/200.9 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python>=3 in /usr/local/lib/python3.10/dist-packages (from mmcv<2.1.0,>=2.0.0rc4->mmpretrain==1.0.0rc8) (4.7.0.72)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from mmengine<1.0.0,>=0.4.0->mmpretrain==1.0.0rc8) (2.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.28.0->mmpretrain==1.0.0rc8) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers>=4.28.0->mmpretrain==1.0.0rc8)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.28.0->mmpretrain==1.0.0rc8) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.28.0->mmpretrain==1.0.0rc8) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers>=4.28.0->mmpretrain==1.0.0rc8)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.28.0->mmpretrain==1.0.0rc8) (4.65.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->mmpretrain==1.0.0rc8) (3.15.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmpretrain==1.0.0rc8) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmpretrain==1.0.0rc8) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmpretrain==1.0.0rc8) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmpretrain==1.0.0rc8) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmpretrain==1.0.0rc8) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmpretrain==1.0.0rc8) (2.8.2)\n",
            "Requirement already satisfied: model-index in /usr/local/lib/python3.10/dist-packages (from modelindex->mmpretrain==1.0.0rc8) (0.1.11)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->mmpretrain==1.0.0rc8) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->mmpretrain==1.0.0rc8) (2.14.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers>=4.28.0->mmpretrain==1.0.0rc8) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers>=4.28.0->mmpretrain==1.0.0rc8) (4.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->mmpretrain==1.0.0rc8) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mmpretrain==1.0.0rc8) (1.16.0)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from model-index->modelindex->mmpretrain==1.0.0rc8) (3.4.3)\n",
            "Requirement already satisfied: ordered-set in /usr/local/lib/python3.10/dist-packages (from model-index->modelindex->mmpretrain==1.0.0rc8) (4.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from model-index->modelindex->mmpretrain==1.0.0rc8) (8.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.28.0->mmpretrain==1.0.0rc8) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.28.0->mmpretrain==1.0.0rc8) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.28.0->mmpretrain==1.0.0rc8) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.28.0->mmpretrain==1.0.0rc8) (3.4)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf->mmcv<2.1.0,>=2.0.0rc4->mmpretrain==1.0.0rc8) (2.0.1)\n",
            "Installing collected packages: tokenizers, addict, yapf, huggingface-hub, transformers, mmengine, mmcv, mmpretrain\n",
            "  Attempting uninstall: mmpretrain\n",
            "    Found existing installation: mmpretrain 1.0.0rc8\n",
            "    Uninstalling mmpretrain-1.0.0rc8:\n",
            "      Successfully uninstalled mmpretrain-1.0.0rc8\n",
            "  Running setup.py develop for mmpretrain\n",
            "Successfully installed addict-2.4.0 huggingface-hub-0.15.1 mmcv-2.0.0 mmengine-0.7.4 mmpretrain-1.0.0rc8 tokenizers-0.13.3 transformers-4.29.2 yapf-0.33.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j55eUA4q-f6S",
        "outputId": "cb2813f9-2c9e-48e3-8157-fa60993a5bdd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.27.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.65.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.4.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/mmpretrain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhKjp4LMgBHd",
        "outputId": "77574c7b-f199-4a57-80b7-50495a738000"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mmpretrain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#创建data文件夹\n",
        "!mkdir data"
      ],
      "metadata": {
        "id": "mElSo_Xsf02e"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#从Google云端硬盘下载数据集fruit30_trian.tar\n",
        "!gdown https://drive.google.com/u/0/uc?id=16AvoLcKzCfvsKFRVQ-RGVnsTRWzAe7ze&export=download"
      ],
      "metadata": {
        "id": "g2lPVK783Q-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4fa6fc6-7253-4105-95a3-2ff47e3bb42c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=16AvoLcKzCfvsKFRVQ-RGVnsTRWzAe7ze\n",
            "To: /content/mmpretrain/fruit30_train.tar\n",
            "100% 187M/187M [00:07<00:00, 25.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#将下载的tar数据集 解压到data所对应的目录下\n",
        "import tarfile\n",
        "\n",
        "tar_path = \"/content/mmpretrain/fruit30_train.tar\"  # Path to your tar file\n",
        "destination_path = \"/content/mmpretrain/data/fruits\"  # Path to the destination folder\n",
        "\n",
        "with tarfile.open(tar_path) as tar:\n",
        "    tar.extractall(path=destination_path)\n"
      ],
      "metadata": {
        "id": "wBT09xxogZ7y"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**数据集划分**"
      ],
      "metadata": {
        "id": "OhMkbesigrPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "# File Location\n",
        "data_path = \"/content/mmpretrain/data/fruits\"  # 源数据路径\n",
        "train_path = \"./data/train\"  # 训练集路径\n",
        "valid_path = \"./data/valid\"  # 验证集路径\n",
        "test_path = \"./data/test\"  # 测试集路径\n",
        " \n",
        "# 创建train, valid, test文件夹\n",
        "os.makedirs(train_path, exist_ok=True)\n",
        "os.makedirs(valid_path, exist_ok=True)\n",
        "os.makedirs(test_path, exist_ok=True)\n",
        " \n",
        "# 遍历data文件夹下的每个子文件夹\n",
        "for folder in os.listdir(data_path):\n",
        "    folder_path = os.path.join(data_path, folder)\n",
        "    if os.path.isdir(folder_path):\n",
        "        # 创建相应的子文件夹\n",
        "        os.makedirs(os.path.join(train_path, folder), exist_ok=True)\n",
        "        os.makedirs(os.path.join(valid_path, folder), exist_ok=True)\n",
        "        os.makedirs(os.path.join(test_path, folder), exist_ok=True)\n",
        " \n",
        "        # 获取子文件夹中的文件列表\n",
        "        files = os.listdir(folder_path)\n",
        "        # 打乱文件顺序！！！！！！！！！！！！！！！\n",
        "        random.shuffle(files)\n",
        " \n",
        "        # 计算训练、验证、测试集的文件数量\n",
        "        train_count = int(len(files) * 0.7)\n",
        "        valid_count = int(len(files) * 0.2)\n",
        "        test_count = len(files) - train_count - valid_count\n",
        " \n",
        "        # 分别划分训练、验证、测试集\n",
        "        train_files = files[:train_count]\n",
        "        valid_files = files[train_count:(train_count+valid_count)]\n",
        "        test_files = files[(train_count+valid_count):]\n",
        " \n",
        "        # 移动文件\n",
        "        for file in train_files:\n",
        "            shutil.move(os.path.join(folder_path, file), os.path.join(train_path, folder, file))\n",
        "        for file in valid_files:\n",
        "            shutil.move(os.path.join(folder_path, file), os.path.join(valid_path, folder, file))\n",
        "        for file in test_files:\n",
        "            shutil.move(os.path.join(folder_path, file), os.path.join(test_path, folder, file))\n",
        " \n",
        " "
      ],
      "metadata": {
        "id": "d0x6q_euguIN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Find Configs 文件**"
      ],
      "metadata": {
        "id": "ItFseG77Z0hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/mmpretrain/tools/train.py /content/mmpretrain/configs/convmixer/convmixer-768-32_10xb64_in1k.py"
      ],
      "metadata": {
        "id": "18rxHaGa8m99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd4df0b1-20ef-4dce-feb1-3fc9fd8ec0d5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "06/07 16:05:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
            "------------------------------------------------------------\n",
            "System environment:\n",
            "    sys.platform: linux\n",
            "    Python: 3.10.11 (main, Apr  5 2023, 14:15:10) [GCC 9.4.0]\n",
            "    CUDA available: True\n",
            "    numpy_random_seed: 1636626534\n",
            "    GPU 0: Tesla T4\n",
            "    CUDA_HOME: /usr/local/cuda\n",
            "    NVCC: Cuda compilation tools, release 11.8, V11.8.89\n",
            "    GCC: x86_64-linux-gnu-gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "    PyTorch: 2.0.1+cu118\n",
            "    PyTorch compiling details: PyTorch built with:\n",
            "  - GCC 9.3\n",
            "  - C++ Version: 201703\n",
            "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 11.8\n",
            "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
            "  - CuDNN 8.7\n",
            "  - Magma 2.6.1\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
            "\n",
            "    TorchVision: 0.15.2+cu118\n",
            "    OpenCV: 4.7.0\n",
            "    MMEngine: 0.7.4\n",
            "\n",
            "Runtime environment:\n",
            "    cudnn_benchmark: False\n",
            "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
            "    dist_cfg: {'backend': 'nccl'}\n",
            "    seed: 1636626534\n",
            "    deterministic: False\n",
            "    Distributed launcher: none\n",
            "    Distributed training: False\n",
            "    GPU number: 1\n",
            "------------------------------------------------------------\n",
            "\n",
            "06/07 16:05:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Config:\n",
            "model = dict(\n",
            "    type='ImageClassifier',\n",
            "    backbone=dict(type='ConvMixer', arch='768/32', act_cfg=dict(type='ReLU')),\n",
            "    neck=dict(type='GlobalAveragePooling'),\n",
            "    head=dict(\n",
            "        type='LinearClsHead',\n",
            "        num_classes=1000,\n",
            "        in_channels=768,\n",
            "        loss=dict(type='CrossEntropyLoss', loss_weight=1.0)))\n",
            "dataset_type = 'ImageNet'\n",
            "data_preprocessor = dict(\n",
            "    num_classes=1000,\n",
            "    mean=[123.675, 116.28, 103.53],\n",
            "    std=[58.395, 57.12, 57.375],\n",
            "    to_rgb=True)\n",
            "bgr_mean = [103.53, 116.28, 123.675]\n",
            "bgr_std = [57.375, 57.12, 58.395]\n",
            "train_pipeline = [\n",
            "    dict(type='LoadImageFromFile'),\n",
            "    dict(\n",
            "        type='RandomResizedCrop',\n",
            "        scale=224,\n",
            "        backend='pillow',\n",
            "        interpolation='bicubic'),\n",
            "    dict(type='RandomFlip', prob=0.5, direction='horizontal'),\n",
            "    dict(\n",
            "        type='RandAugment',\n",
            "        policies='timm_increasing',\n",
            "        num_policies=2,\n",
            "        total_level=10,\n",
            "        magnitude_level=9,\n",
            "        magnitude_std=0.5,\n",
            "        hparams=dict(pad_val=[104, 116, 124], interpolation='bicubic')),\n",
            "    dict(\n",
            "        type='RandomErasing',\n",
            "        erase_prob=0.25,\n",
            "        mode='rand',\n",
            "        min_area_ratio=0.02,\n",
            "        max_area_ratio=0.3333333333333333,\n",
            "        fill_color=[103.53, 116.28, 123.675],\n",
            "        fill_std=[57.375, 57.12, 58.395]),\n",
            "    dict(type='PackInputs')\n",
            "]\n",
            "test_pipeline = [\n",
            "    dict(type='LoadImageFromFile'),\n",
            "    dict(\n",
            "        type='ResizeEdge',\n",
            "        scale=233,\n",
            "        edge='short',\n",
            "        backend='pillow',\n",
            "        interpolation='bicubic'),\n",
            "    dict(type='CenterCrop', crop_size=224),\n",
            "    dict(type='PackInputs')\n",
            "]\n",
            "train_dataloader = dict(\n",
            "    pin_memory=True,\n",
            "    persistent_workers=True,\n",
            "    collate_fn=dict(type='default_collate'),\n",
            "    batch_size=64,\n",
            "    num_workers=5,\n",
            "    dataset=dict(\n",
            "        type='ImageNet',\n",
            "        data_root='data/imagenet',\n",
            "        ann_file='meta/train.txt',\n",
            "        data_prefix='train',\n",
            "        pipeline=[\n",
            "            dict(type='LoadImageFromFile'),\n",
            "            dict(\n",
            "                type='RandomResizedCrop',\n",
            "                scale=224,\n",
            "                backend='pillow',\n",
            "                interpolation='bicubic'),\n",
            "            dict(type='RandomFlip', prob=0.5, direction='horizontal'),\n",
            "            dict(\n",
            "                type='RandAugment',\n",
            "                policies='timm_increasing',\n",
            "                num_policies=2,\n",
            "                total_level=10,\n",
            "                magnitude_level=9,\n",
            "                magnitude_std=0.5,\n",
            "                hparams=dict(pad_val=[104, 116, 124],\n",
            "                             interpolation='bicubic')),\n",
            "            dict(\n",
            "                type='RandomErasing',\n",
            "                erase_prob=0.25,\n",
            "                mode='rand',\n",
            "                min_area_ratio=0.02,\n",
            "                max_area_ratio=0.3333333333333333,\n",
            "                fill_color=[103.53, 116.28, 123.675],\n",
            "                fill_std=[57.375, 57.12, 58.395]),\n",
            "            dict(type='PackInputs')\n",
            "        ]),\n",
            "    sampler=dict(type='DefaultSampler', shuffle=True))\n",
            "val_dataloader = dict(\n",
            "    pin_memory=True,\n",
            "    persistent_workers=True,\n",
            "    collate_fn=dict(type='default_collate'),\n",
            "    batch_size=64,\n",
            "    num_workers=5,\n",
            "    dataset=dict(\n",
            "        type='ImageNet',\n",
            "        data_root='data/imagenet',\n",
            "        ann_file='meta/val.txt',\n",
            "        data_prefix='val',\n",
            "        pipeline=[\n",
            "            dict(type='LoadImageFromFile'),\n",
            "            dict(\n",
            "                type='ResizeEdge',\n",
            "                scale=233,\n",
            "                edge='short',\n",
            "                backend='pillow',\n",
            "                interpolation='bicubic'),\n",
            "            dict(type='CenterCrop', crop_size=224),\n",
            "            dict(type='PackInputs')\n",
            "        ]),\n",
            "    sampler=dict(type='DefaultSampler', shuffle=False))\n",
            "val_evaluator = dict(type='Accuracy', topk=(1, 5))\n",
            "test_dataloader = dict(\n",
            "    pin_memory=True,\n",
            "    persistent_workers=True,\n",
            "    collate_fn=dict(type='default_collate'),\n",
            "    batch_size=64,\n",
            "    num_workers=5,\n",
            "    dataset=dict(\n",
            "        type='ImageNet',\n",
            "        data_root='data/imagenet',\n",
            "        ann_file='meta/val.txt',\n",
            "        data_prefix='val',\n",
            "        pipeline=[\n",
            "            dict(type='LoadImageFromFile'),\n",
            "            dict(\n",
            "                type='ResizeEdge',\n",
            "                scale=233,\n",
            "                edge='short',\n",
            "                backend='pillow',\n",
            "                interpolation='bicubic'),\n",
            "            dict(type='CenterCrop', crop_size=224),\n",
            "            dict(type='PackInputs')\n",
            "        ]),\n",
            "    sampler=dict(type='DefaultSampler', shuffle=False))\n",
            "test_evaluator = dict(type='Accuracy', topk=(1, 5))\n",
            "optim_wrapper = dict(\n",
            "    optimizer=dict(\n",
            "        type='AdamW',\n",
            "        lr=0.01,\n",
            "        weight_decay=0.05,\n",
            "        eps=1e-08,\n",
            "        betas=(0.9, 0.999)),\n",
            "    paramwise_cfg=dict(\n",
            "        norm_decay_mult=0.0,\n",
            "        bias_decay_mult=0.0,\n",
            "        flat_decay_mult=0.0,\n",
            "        custom_keys=dict({\n",
            "            '.absolute_pos_embed': dict(decay_mult=0.0),\n",
            "            '.relative_position_bias_table': dict(decay_mult=0.0)\n",
            "        })),\n",
            "    clip_grad=dict(max_norm=5.0))\n",
            "param_scheduler = [\n",
            "    dict(\n",
            "        type='LinearLR',\n",
            "        start_factor=0.001,\n",
            "        by_epoch=True,\n",
            "        end=20,\n",
            "        convert_to_iter_based=True),\n",
            "    dict(type='CosineAnnealingLR', eta_min=1e-05, by_epoch=True, begin=20)\n",
            "]\n",
            "train_cfg = dict(by_epoch=True, max_epochs=300, val_interval=1)\n",
            "val_cfg = dict()\n",
            "test_cfg = dict()\n",
            "auto_scale_lr = dict(base_batch_size=640)\n",
            "default_scope = 'mmpretrain'\n",
            "default_hooks = dict(\n",
            "    timer=dict(type='IterTimerHook'),\n",
            "    logger=dict(type='LoggerHook', interval=100),\n",
            "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
            "    checkpoint=dict(type='CheckpointHook', interval=1),\n",
            "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
            "    visualization=dict(type='VisualizationHook', enable=False))\n",
            "env_cfg = dict(\n",
            "    cudnn_benchmark=False,\n",
            "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),\n",
            "    dist_cfg=dict(backend='nccl'))\n",
            "vis_backends = [dict(type='LocalVisBackend')]\n",
            "visualizer = dict(\n",
            "    type='UniversalVisualizer', vis_backends=[dict(type='LocalVisBackend')])\n",
            "log_level = 'INFO'\n",
            "load_from = None\n",
            "resume = False\n",
            "randomness = dict(seed=None, deterministic=False)\n",
            "launcher = 'none'\n",
            "work_dir = './work_dirs/convmixer-768-32_10xb64_in1k'\n",
            "\n",
            "06/07 16:05:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
            "06/07 16:05:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
            "before_run:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "before_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_train_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) DistSamplerSeedHook                \n",
            " -------------------- \n",
            "before_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_train_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_val_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "before_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) VisualizationHook                  \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_val_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_train:\n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_test_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "before_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) VisualizationHook                  \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_run:\n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mmengine/registry/build_functions.py\", line 122, in build_from_cfg\n",
            "    obj = obj_cls(**args)  # type: ignore\n",
            "  File \"/content/mmpretrain/mmpretrain/datasets/imagenet.py\", line 39, in __init__\n",
            "    super().__init__(\n",
            "  File \"/content/mmpretrain/mmpretrain/datasets/custom.py\", line 219, in __init__\n",
            "    self.full_init()\n",
            "  File \"/content/mmpretrain/mmpretrain/datasets/base_dataset.py\", line 178, in full_init\n",
            "    super().full_init()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mmengine/dataset/base_dataset.py\", line 296, in full_init\n",
            "    self.data_list = self.load_data_list()\n",
            "  File \"/content/mmpretrain/mmpretrain/datasets/custom.py\", line 266, in load_data_list\n",
            "    lines = list_from_file(self.ann_file)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mmengine/fileio/parse.py\", line 60, in list_from_file\n",
            "    text = get_text(filename, encoding, backend_args=backend_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mmengine/fileio/io.py\", line 208, in get_text\n",
            "    return backend.get_text(filepath, encoding)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mmengine/fileio/backends/local_backend.py\", line 56, in get_text\n",
            "    with open(filepath, encoding=encoding) as f:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'data/imagenet/meta/train.txt'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/mmpretrain/tools/train.py\", line 159, in <module>\n",
            "    main()\n",
            "  File \"/content/mmpretrain/tools/train.py\", line 155, in main\n",
            "    runner.train()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mmengine/runner/runner.py\", line 1687, in train\n",
            "    self._train_loop = self.build_train_loop(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mmengine/runner/runner.py\", line 1486, in build_train_loop\n",
            "    loop = EpochBasedTrainLoop(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mmengine/runner/loops.py\", line 44, in __init__\n",
            "    super().__init__(runner, dataloader)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mmengine/runner/base_loop.py\", line 26, in __init__\n",
            "    self.dataloader = runner.build_dataloader(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mmengine/runner/runner.py\", line 1346, in build_dataloader\n",
            "    dataset = DATASETS.build(dataset_cfg)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mmengine/registry/registry.py\", line 548, in build\n",
            "    return self.build_func(cfg, *args, **kwargs, registry=self)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mmengine/registry/build_functions.py\", line 144, in build_from_cfg\n",
            "    raise type(e)(\n",
            "FileNotFoundError: class `ImageNet` in mmpretrain/datasets/imagenet.py: [Errno 2] No such file or directory: 'data/imagenet/meta/train.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/mmpretrain/tools/train.py /content/mmpretrain/work_dirs/convmixer-768-32_10xb64_in1k/convmixer-768-32_10xb64_in1k.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3XK8JmpabKQ",
        "outputId": "02bee3be-d653-4fe7-8514-b901b3056519"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "06/07 17:19:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
            "------------------------------------------------------------\n",
            "System environment:\n",
            "    sys.platform: linux\n",
            "    Python: 3.10.11 (main, Apr  5 2023, 14:15:10) [GCC 9.4.0]\n",
            "    CUDA available: True\n",
            "    numpy_random_seed: 196464682\n",
            "    GPU 0: Tesla T4\n",
            "    CUDA_HOME: /usr/local/cuda\n",
            "    NVCC: Cuda compilation tools, release 11.8, V11.8.89\n",
            "    GCC: x86_64-linux-gnu-gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "    PyTorch: 2.0.1+cu118\n",
            "    PyTorch compiling details: PyTorch built with:\n",
            "  - GCC 9.3\n",
            "  - C++ Version: 201703\n",
            "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 11.8\n",
            "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
            "  - CuDNN 8.7\n",
            "  - Magma 2.6.1\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
            "\n",
            "    TorchVision: 0.15.2+cu118\n",
            "    OpenCV: 4.7.0\n",
            "    MMEngine: 0.7.4\n",
            "\n",
            "Runtime environment:\n",
            "    cudnn_benchmark: False\n",
            "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
            "    dist_cfg: {'backend': 'nccl'}\n",
            "    seed: 196464682\n",
            "    deterministic: False\n",
            "    Distributed launcher: none\n",
            "    Distributed training: False\n",
            "    GPU number: 1\n",
            "------------------------------------------------------------\n",
            "\n",
            "06/07 17:19:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Config:\n",
            "model = dict(\n",
            "    type='ImageClassifier',\n",
            "    backbone=dict(type='ConvMixer', arch='768/32', act_cfg=dict(type='ReLU')),\n",
            "    neck=dict(type='GlobalAveragePooling'),\n",
            "    head=dict(\n",
            "        type='LinearClsHead',\n",
            "        num_classes=30,\n",
            "        in_channels=768,\n",
            "        loss=dict(type='CrossEntropyLoss', loss_weight=1.0)))\n",
            "dataset_type = 'CustomDataset'\n",
            "data_preprocessor = dict(\n",
            "    num_classes=30,\n",
            "    mean=[123.675, 116.28, 103.53],\n",
            "    std=[58.395, 57.12, 57.375],\n",
            "    to_rgb=True)\n",
            "bgr_mean = [103.53, 116.28, 123.675]\n",
            "bgr_std = [57.375, 57.12, 58.395]\n",
            "train_pipeline = [\n",
            "    dict(type='LoadImageFromFile'),\n",
            "    dict(\n",
            "        type='RandomResizedCrop',\n",
            "        scale=224,\n",
            "        backend='pillow',\n",
            "        interpolation='bicubic'),\n",
            "    dict(type='RandomFlip', prob=0.5, direction='horizontal'),\n",
            "    dict(\n",
            "        type='RandAugment',\n",
            "        policies='timm_increasing',\n",
            "        num_policies=2,\n",
            "        total_level=10,\n",
            "        magnitude_level=9,\n",
            "        magnitude_std=0.5,\n",
            "        hparams=dict(pad_val=[104, 116, 124], interpolation='bicubic')),\n",
            "    dict(\n",
            "        type='RandomErasing',\n",
            "        erase_prob=0.25,\n",
            "        mode='rand',\n",
            "        min_area_ratio=0.02,\n",
            "        max_area_ratio=0.3333333333333333,\n",
            "        fill_color=[103.53, 116.28, 123.675],\n",
            "        fill_std=[57.375, 57.12, 58.395]),\n",
            "    dict(type='PackInputs')\n",
            "]\n",
            "test_pipeline = [\n",
            "    dict(type='LoadImageFromFile'),\n",
            "    dict(\n",
            "        type='ResizeEdge',\n",
            "        scale=233,\n",
            "        edge='short',\n",
            "        backend='pillow',\n",
            "        interpolation='bicubic'),\n",
            "    dict(type='CenterCrop', crop_size=224),\n",
            "    dict(type='PackInputs')\n",
            "]\n",
            "train_dataloader = dict(\n",
            "    pin_memory=True,\n",
            "    persistent_workers=True,\n",
            "    collate_fn=dict(type='default_collate'),\n",
            "    batch_size=32,\n",
            "    num_workers=5,\n",
            "    dataset=dict(\n",
            "        type='CustomDataset',\n",
            "        data_root='/content/mmpretrain/data/train',\n",
            "        pipeline=[\n",
            "            dict(type='LoadImageFromFile'),\n",
            "            dict(\n",
            "                type='RandomResizedCrop',\n",
            "                scale=224,\n",
            "                backend='pillow',\n",
            "                interpolation='bicubic'),\n",
            "            dict(type='RandomFlip', prob=0.5, direction='horizontal'),\n",
            "            dict(\n",
            "                type='RandAugment',\n",
            "                policies='timm_increasing',\n",
            "                num_policies=2,\n",
            "                total_level=10,\n",
            "                magnitude_level=9,\n",
            "                magnitude_std=0.5,\n",
            "                hparams=dict(pad_val=[104, 116, 124],\n",
            "                             interpolation='bicubic')),\n",
            "            dict(\n",
            "                type='RandomErasing',\n",
            "                erase_prob=0.25,\n",
            "                mode='rand',\n",
            "                min_area_ratio=0.02,\n",
            "                max_area_ratio=0.3333333333333333,\n",
            "                fill_color=[103.53, 116.28, 123.675],\n",
            "                fill_std=[57.375, 57.12, 58.395]),\n",
            "            dict(type='PackInputs')\n",
            "        ]),\n",
            "    sampler=dict(type='DefaultSampler', shuffle=True))\n",
            "val_dataloader = dict(\n",
            "    pin_memory=True,\n",
            "    persistent_workers=True,\n",
            "    collate_fn=dict(type='default_collate'),\n",
            "    batch_size=32,\n",
            "    num_workers=5,\n",
            "    dataset=dict(\n",
            "        type='CustomDataset',\n",
            "        data_root='/content/mmpretrain/data/valid',\n",
            "        pipeline=[\n",
            "            dict(type='LoadImageFromFile'),\n",
            "            dict(\n",
            "                type='ResizeEdge',\n",
            "                scale=233,\n",
            "                edge='short',\n",
            "                backend='pillow',\n",
            "                interpolation='bicubic'),\n",
            "            dict(type='CenterCrop', crop_size=224),\n",
            "            dict(type='PackInputs')\n",
            "        ]),\n",
            "    sampler=dict(type='DefaultSampler', shuffle=False))\n",
            "val_evaluator = dict(type='Accuracy', topk=1)\n",
            "test_dataloader = dict(\n",
            "    pin_memory=True,\n",
            "    persistent_workers=True,\n",
            "    collate_fn=dict(type='default_collate'),\n",
            "    batch_size=32,\n",
            "    num_workers=5,\n",
            "    dataset=dict(\n",
            "        type='CustomDataset',\n",
            "        data_root='/content/mmpretrain/data/test',\n",
            "        pipeline=[\n",
            "            dict(type='LoadImageFromFile'),\n",
            "            dict(\n",
            "                type='ResizeEdge',\n",
            "                scale=233,\n",
            "                edge='short',\n",
            "                backend='pillow',\n",
            "                interpolation='bicubic'),\n",
            "            dict(type='CenterCrop', crop_size=224),\n",
            "            dict(type='PackInputs')\n",
            "        ]),\n",
            "    sampler=dict(type='DefaultSampler', shuffle=False))\n",
            "test_evaluator = dict(type='Accuracy', topk=1)\n",
            "optim_wrapper = dict(\n",
            "    optimizer=dict(\n",
            "        type='AdamW',\n",
            "        lr=0.01,\n",
            "        weight_decay=0.05,\n",
            "        eps=1e-08,\n",
            "        betas=(0.9, 0.999)),\n",
            "    paramwise_cfg=dict(\n",
            "        norm_decay_mult=0.0,\n",
            "        bias_decay_mult=0.0,\n",
            "        flat_decay_mult=0.0,\n",
            "        custom_keys=dict({\n",
            "            '.absolute_pos_embed': dict(decay_mult=0.0),\n",
            "            '.relative_position_bias_table': dict(decay_mult=0.0)\n",
            "        })),\n",
            "    clip_grad=dict(max_norm=5.0))\n",
            "param_scheduler = [\n",
            "    dict(\n",
            "        type='LinearLR',\n",
            "        start_factor=0.001,\n",
            "        by_epoch=True,\n",
            "        end=20,\n",
            "        convert_to_iter_based=True),\n",
            "    dict(type='CosineAnnealingLR', eta_min=1e-05, by_epoch=True, begin=20)\n",
            "]\n",
            "train_cfg = dict(by_epoch=True, max_epochs=300, val_interval=1)\n",
            "val_cfg = dict()\n",
            "test_cfg = dict()\n",
            "auto_scale_lr = dict(base_batch_size=640)\n",
            "default_scope = 'mmpretrain'\n",
            "default_hooks = dict(\n",
            "    timer=dict(type='IterTimerHook'),\n",
            "    logger=dict(type='LoggerHook', interval=5),\n",
            "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
            "    checkpoint=dict(type='CheckpointHook', interval=1),\n",
            "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
            "    visualization=dict(type='VisualizationHook', enable=False))\n",
            "env_cfg = dict(\n",
            "    cudnn_benchmark=False,\n",
            "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),\n",
            "    dist_cfg=dict(backend='nccl'))\n",
            "vis_backends = [dict(type='LocalVisBackend')]\n",
            "visualizer = dict(\n",
            "    type='UniversalVisualizer', vis_backends=[dict(type='LocalVisBackend')])\n",
            "log_level = 'INFO'\n",
            "load_from = 'https://download.openmmlab.com/mmclassification/v0/convmixer/convmixer-768-32_3rdparty_10xb64_in1k_20220323-bca1f7b8.pth'\n",
            "resume = False\n",
            "randomness = dict(seed=None, deterministic=False)\n",
            "launcher = 'none'\n",
            "work_dir = './work_dirs/convmixer-768-32_10xb64_in1k'\n",
            "\n",
            "06/07 17:20:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
            "06/07 17:20:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
            "before_run:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "before_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_train_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) DistSamplerSeedHook                \n",
            " -------------------- \n",
            "before_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_train_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_val_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "before_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) VisualizationHook                  \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_val_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_train:\n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_test_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "before_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) VisualizationHook                  \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_run:\n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stem.0.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stem.2.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stem.2.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.0.0.fn.0.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.0.0.fn.2.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.0.0.fn.2.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.0.1.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.0.3.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.0.3.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.1.0.fn.0.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.1.0.fn.2.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.1.0.fn.2.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.1.1.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.1.3.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.1.3.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.2.0.fn.0.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.2.0.fn.2.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.2.0.fn.2.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.2.1.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.2.3.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.2.3.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.3.0.fn.0.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.3.0.fn.2.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.3.0.fn.2.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.3.1.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.3.3.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.3.3.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.4.0.fn.0.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.4.0.fn.2.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.4.0.fn.2.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.4.1.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.4.3.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.4.3.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.5.0.fn.0.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.5.0.fn.2.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.5.0.fn.2.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.5.1.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.5.3.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.5.3.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.6.0.fn.0.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.6.0.fn.2.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.6.0.fn.2.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.6.1.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.6.3.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.6.3.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.7.0.fn.0.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.7.0.fn.2.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.7.0.fn.2.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.7.1.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.7.3.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.7.3.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.8.0.fn.0.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.8.0.fn.2.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.8.0.fn.2.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.8.1.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.8.3.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.8.3.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.9.0.fn.0.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.9.0.fn.2.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.9.0.fn.2.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.9.1.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.9.3.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.9.3.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.10.0.fn.0.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.10.0.fn.2.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.10.0.fn.2.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.10.1.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.10.3.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.10.3.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.11.0.fn.0.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.11.0.fn.2.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.11.0.fn.2.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.11.1.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.11.3.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.11.3.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.12.0.fn.0.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.12.0.fn.2.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.12.0.fn.2.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.12.1.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.12.3.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.12.3.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.13.0.fn.0.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.13.0.fn.2.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.13.0.fn.2.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.13.1.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.13.3.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.13.3.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.14.0.fn.0.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.14.0.fn.2.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.14.0.fn.2.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.14.1.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.14.3.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.14.3.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.15.0.fn.0.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.15.0.fn.2.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.15.0.fn.2.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.15.1.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.15.3.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.15.3.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.16.0.fn.0.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.16.0.fn.2.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.16.0.fn.2.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.16.1.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.16.3.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.16.3.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.17.0.fn.0.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.17.0.fn.2.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.17.0.fn.2.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.17.1.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.17.3.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.17.3.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.18.0.fn.0.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.18.0.fn.2.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.18.0.fn.2.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.18.1.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.18.3.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.18.3.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.19.0.fn.0.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.19.0.fn.2.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.19.0.fn.2.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.19.1.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.19.3.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.19.3.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.20.0.fn.0.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.20.0.fn.2.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.20.0.fn.2.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.20.1.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.20.3.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.20.3.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.21.0.fn.0.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.21.0.fn.2.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.21.0.fn.2.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.21.1.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.21.3.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.21.3.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.22.0.fn.0.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.22.0.fn.2.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.22.0.fn.2.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.22.1.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.22.3.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.22.3.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.23.0.fn.0.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.23.0.fn.2.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.23.0.fn.2.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.23.1.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.23.3.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.23.3.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.24.0.fn.0.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.24.0.fn.2.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.24.0.fn.2.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.24.1.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.24.3.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.24.3.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.25.0.fn.0.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.25.0.fn.2.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.25.0.fn.2.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.25.1.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.25.3.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.25.3.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.26.0.fn.0.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.26.0.fn.2.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.26.0.fn.2.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.26.1.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.26.3.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.26.3.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.27.0.fn.0.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.27.0.fn.2.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.27.0.fn.2.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.27.1.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.27.3.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.27.3.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.28.0.fn.0.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.28.0.fn.2.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.28.0.fn.2.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.28.1.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.28.3.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.28.3.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.29.0.fn.0.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.29.0.fn.2.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.29.0.fn.2.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.29.1.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.29.3.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.29.3.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.30.0.fn.0.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.30.0.fn.2.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.30.0.fn.2.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.30.1.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.30.3.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.30.3.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.31.0.fn.0.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.31.0.fn.2.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.31.0.fn.2.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.31.1.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.31.3.weight:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.stages.31.3.bias:weight_decay=0.0\n",
            "06/07 17:20:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- head.fc.bias:weight_decay=0.0\n",
            "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/convmixer/convmixer-768-32_3rdparty_10xb64_in1k_20220323-bca1f7b8.pth\n",
            "Downloading: \"https://download.openmmlab.com/mmclassification/v0/convmixer/convmixer-768-32_3rdparty_10xb64_in1k_20220323-bca1f7b8.pth\" to /root/.cache/torch/hub/checkpoints/convmixer-768-32_3rdparty_10xb64_in1k_20220323-bca1f7b8.pth\n",
            "100% 81.0M/81.0M [00:12<00:00, 6.56MB/s]\n",
            "The model and loaded state dict do not match exactly\n",
            "\n",
            "size mismatch for head.fc.weight: copying a param with shape torch.Size([1000, 768]) from checkpoint, the shape in current model is torch.Size([30, 768]).\n",
            "size mismatch for head.fc.bias: copying a param with shape torch.Size([1000]) from checkpoint, the shape in current model is torch.Size([30]).\n",
            "06/07 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Load checkpoint from https://download.openmmlab.com/mmclassification/v0/convmixer/convmixer-768-32_3rdparty_10xb64_in1k_20220323-bca1f7b8.pth\n",
            "06/07 17:20:18 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
            "06/07 17:20:18 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
            "06/07 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to /content/mmpretrain/work_dirs/convmixer-768-32_10xb64_in1k.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "06/07 17:20:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [1][ 5/96]  lr: 3.0823e-05  eta: 1 day, 5:06:17  time: 3.6388  data_time: 0.3404  memory: 12844  grad_norm: 0.8632  loss: 3.3965\n",
            "06/07 17:20:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [1][10/96]  lr: 5.6853e-05  eta: 1 day, 0:06:26  time: 3.0145  data_time: 0.1722  memory: 12844  grad_norm: 0.8690  loss: 3.3972\n",
            "06/07 17:21:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [1][15/96]  lr: 8.2882e-05  eta: 22:42:37  time: 2.4410  data_time: 0.0032  memory: 12844  grad_norm: 0.8715  loss: 3.3979\n",
            "06/07 17:21:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [1][20/96]  lr: 1.0891e-04  eta: 22:07:18  time: 2.5199  data_time: 0.0040  memory: 12844  grad_norm: 0.8731  loss: 3.3873\n",
            "06/07 17:21:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [1][25/96]  lr: 1.3494e-04  eta: 21:36:13  time: 2.4966  data_time: 0.0040  memory: 12844  grad_norm: 0.8937  loss: 3.3735\n",
            "06/07 17:21:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [1][30/96]  lr: 1.6097e-04  eta: 21:10:03  time: 2.4119  data_time: 0.0020  memory: 12844  grad_norm: 0.9343  loss: 3.3600\n",
            "06/07 17:21:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [1][35/96]  lr: 1.8700e-04  eta: 20:50:07  time: 2.3696  data_time: 0.0023  memory: 12844  grad_norm: 1.0136  loss: 3.3363\n",
            "06/07 17:22:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [1][40/96]  lr: 2.1303e-04  eta: 20:34:59  time: 2.3597  data_time: 0.0028  memory: 12844  grad_norm: 1.1806  loss: 3.2948\n",
            "06/07 17:22:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [1][45/96]  lr: 2.3906e-04  eta: 20:24:44  time: 2.3733  data_time: 0.0022  memory: 12844  grad_norm: 1.4643  loss: 3.2184\n",
            "06/07 17:22:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [1][50/96]  lr: 2.6509e-04  eta: 20:17:34  time: 2.3993  data_time: 0.0022  memory: 12844  grad_norm: 1.8794  loss: 3.1182\n",
            "06/07 17:22:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [1][55/96]  lr: 2.9112e-04  eta: 20:12:41  time: 2.4221  data_time: 0.0040  memory: 12844  grad_norm: 2.2205  loss: 2.9752\n",
            "06/07 17:22:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [1][60/96]  lr: 3.1714e-04  eta: 20:08:34  time: 2.4335  data_time: 0.0040  memory: 12844  grad_norm: 2.4825  loss: 2.7982\n",
            "06/07 17:23:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [1][65/96]  lr: 3.4317e-04  eta: 20:04:24  time: 2.4246  data_time: 0.0032  memory: 12844  grad_norm: 2.7538  loss: 2.5787\n",
            "06/07 17:23:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [1][70/96]  lr: 3.6920e-04  eta: 20:00:02  time: 2.4044  data_time: 0.0033  memory: 12844  grad_norm: 2.8628  loss: 2.3330\n",
            "06/07 17:23:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [1][75/96]  lr: 3.9523e-04  eta: 19:55:29  time: 2.3818  data_time: 0.0024  memory: 12844  grad_norm: 2.8851  loss: 2.1063\n",
            "06/07 17:23:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [1][80/96]  lr: 4.2126e-04  eta: 19:52:03  time: 2.3799  data_time: 0.0016  memory: 12844  grad_norm: 2.9123  loss: 1.9171\n",
            "06/07 17:23:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [1][85/96]  lr: 4.4729e-04  eta: 19:49:20  time: 2.3955  data_time: 0.0022  memory: 12844  grad_norm: 2.9428  loss: 1.7604\n",
            "06/07 17:24:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [1][90/96]  lr: 4.7332e-04  eta: 19:47:10  time: 2.4063  data_time: 0.0021  memory: 12844  grad_norm: 2.8800  loss: 1.5723\n",
            "06/07 17:24:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [1][95/96]  lr: 4.9935e-04  eta: 19:45:04  time: 2.4085  data_time: 0.0013  memory: 12844  grad_norm: 2.8674  loss: 1.4238\n",
            "06/07 17:24:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: convmixer-768-32_10xb64_in1k_20230607_171958\n",
            "06/07 17:24:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 1 epochs\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "06/07 17:24:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [1][ 5/27]    eta: 0:00:23  time: 1.0493  data_time: 0.3380  memory: 4674  \n",
            "06/07 17:24:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [1][10/27]    eta: 0:00:14  time: 0.8781  data_time: 0.1695  memory: 558  \n",
            "06/07 17:24:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [1][15/27]    eta: 0:00:09  time: 0.7079  data_time: 0.0009  memory: 558  \n",
            "06/07 17:24:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [1][20/27]    eta: 0:00:05  time: 0.7078  data_time: 0.0015  memory: 558  \n",
            "06/07 17:24:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [1][25/27]    eta: 0:00:01  time: 0.7071  data_time: 0.0015  memory: 558  \n",
            "06/07 17:24:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][27/27]    accuracy/top1: 74.5940  data_time: 0.0635  time: 0.7690\n",
            "06/07 17:24:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [2][ 5/96]  lr: 5.3058e-04  eta: 19:40:49  time: 2.3638  data_time: 0.1146  memory: 12844  grad_norm: 3.0574  loss: 1.3802\n",
            "06/07 17:25:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [2][10/96]  lr: 5.5661e-04  eta: 19:39:11  time: 2.5184  data_time: 0.1148  memory: 12844  grad_norm: 2.8015  loss: 1.2632\n",
            "06/07 17:25:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [2][15/96]  lr: 5.8264e-04  eta: 19:37:50  time: 2.4088  data_time: 0.0033  memory: 12844  grad_norm: 6.8732  loss: 1.2523\n",
            "06/07 17:25:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [2][20/96]  lr: 6.0867e-04  eta: 19:36:35  time: 2.4126  data_time: 0.0051  memory: 12844  grad_norm: 6.7722  loss: 1.1669\n",
            "06/07 17:25:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [2][25/96]  lr: 6.3470e-04  eta: 19:35:17  time: 2.4094  data_time: 0.0040  memory: 12844  grad_norm: 2.6863  loss: 1.1212\n",
            "06/07 17:25:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [2][30/96]  lr: 6.6073e-04  eta: 19:34:02  time: 2.4048  data_time: 0.0030  memory: 12844  grad_norm: 2.6450  loss: 1.0580\n",
            "06/07 17:26:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [2][35/96]  lr: 6.8676e-04  eta: 19:32:47  time: 2.4014  data_time: 0.0033  memory: 12844  grad_norm: 2.6305  loss: 1.0089\n",
            "06/07 17:26:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [2][40/96]  lr: 7.1279e-04  eta: 19:31:41  time: 2.4015  data_time: 0.0033  memory: 12844  grad_norm: 2.7492  loss: 1.0425\n",
            "06/07 17:26:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [2][45/96]  lr: 7.3882e-04  eta: 19:30:39  time: 2.4036  data_time: 0.0031  memory: 12844  grad_norm: 2.7586  loss: 1.0247\n",
            "06/07 17:26:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [2][50/96]  lr: 7.6485e-04  eta: 19:29:35  time: 2.4009  data_time: 0.0032  memory: 12844  grad_norm: 2.7325  loss: 1.0384\n",
            "06/07 17:26:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [2][55/96]  lr: 7.9088e-04  eta: 19:28:26  time: 2.3935  data_time: 0.0026  memory: 12844  grad_norm: 2.9072  loss: 0.9752\n",
            "06/07 17:27:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [2][60/96]  lr: 8.1690e-04  eta: 19:27:28  time: 2.3933  data_time: 0.0031  memory: 12844  grad_norm: 2.8773  loss: 0.9082\n",
            "06/07 17:27:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [2][65/96]  lr: 8.4293e-04  eta: 19:26:36  time: 2.3993  data_time: 0.0042  memory: 12844  grad_norm: 2.7940  loss: 1.0563\n",
            "06/07 17:27:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [2][70/96]  lr: 8.6896e-04  eta: 19:25:47  time: 2.4007  data_time: 0.0035  memory: 12844  grad_norm: 2.7990  loss: 1.0823\n",
            "06/07 17:27:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [2][75/96]  lr: 8.9499e-04  eta: 19:24:53  time: 2.3969  data_time: 0.0029  memory: 12844  grad_norm: 2.7990  loss: 1.0382\n",
            "06/07 17:27:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [2][80/96]  lr: 9.2102e-04  eta: 19:24:12  time: 2.3999  data_time: 0.0039  memory: 12844  grad_norm: 2.6140  loss: 0.9895\n",
            "06/07 17:28:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [2][85/96]  lr: 9.4705e-04  eta: 19:23:27  time: 2.4028  data_time: 0.0039  memory: 12844  grad_norm: 2.6021  loss: 1.0108\n",
            "06/07 17:28:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [2][90/96]  lr: 9.7308e-04  eta: 19:22:50  time: 2.4024  data_time: 0.0022  memory: 12844  grad_norm: 2.7000  loss: 0.9787\n",
            "06/07 17:28:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [2][95/96]  lr: 9.9911e-04  eta: 19:22:02  time: 2.3983  data_time: 0.0014  memory: 12844  grad_norm: 2.6751  loss: 1.0069\n",
            "06/07 17:28:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: convmixer-768-32_10xb64_in1k_20230607_171958\n",
            "06/07 17:28:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 2 epochs\n",
            "06/07 17:28:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [2][ 5/27]    eta: 0:00:21  time: 0.8382  data_time: 0.1329  memory: 4674  \n",
            "06/07 17:28:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [2][10/27]    eta: 0:00:14  time: 0.8431  data_time: 0.1330  memory: 558  \n",
            "06/07 17:28:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [2][15/27]    eta: 0:00:09  time: 0.7076  data_time: 0.0009  memory: 558  \n",
            "06/07 17:28:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [2][20/27]    eta: 0:00:05  time: 0.7082  data_time: 0.0008  memory: 558  \n",
            "06/07 17:28:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [2][25/27]    eta: 0:00:01  time: 0.7086  data_time: 0.0007  memory: 558  \n",
            "06/07 17:28:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][27/27]    accuracy/top1: 84.4548  data_time: 0.0480  time: 0.7529\n",
            "06/07 17:29:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [3][ 5/96]  lr: 1.0303e-03  eta: 19:20:35  time: 2.3684  data_time: 0.1191  memory: 12844  grad_norm: 2.6826  loss: 1.0846\n",
            "06/07 17:29:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [3][10/96]  lr: 1.0564e-03  eta: 19:20:10  time: 2.5357  data_time: 0.1193  memory: 12844  grad_norm: 2.4199  loss: 0.9289\n",
            "06/07 17:29:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [3][15/96]  lr: 1.0824e-03  eta: 19:19:42  time: 2.4140  data_time: 0.0016  memory: 12844  grad_norm: 2.5284  loss: 0.9513\n",
            "06/07 17:29:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [3][20/96]  lr: 1.1084e-03  eta: 19:19:11  time: 2.4083  data_time: 0.0023  memory: 12844  grad_norm: 2.4250  loss: 0.8785\n",
            "06/07 17:29:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [3][25/96]  lr: 1.1345e-03  eta: 19:18:36  time: 2.4017  data_time: 0.0030  memory: 12844  grad_norm: 2.4428  loss: 0.8420\n",
            "06/07 17:30:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [3][30/96]  lr: 1.1605e-03  eta: 19:17:56  time: 2.3937  data_time: 0.0029  memory: 12844  grad_norm: 2.5116  loss: 0.8961\n",
            "06/07 17:30:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [3][35/96]  lr: 1.1865e-03  eta: 19:17:19  time: 2.3902  data_time: 0.0031  memory: 12844  grad_norm: 2.4912  loss: 0.8892\n",
            "06/07 17:30:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [3][40/96]  lr: 1.2125e-03  eta: 19:16:52  time: 2.3982  data_time: 0.0036  memory: 12844  grad_norm: 2.5372  loss: 0.8661\n",
            "06/07 17:30:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [3][45/96]  lr: 1.2386e-03  eta: 19:16:30  time: 2.4094  data_time: 0.0035  memory: 12844  grad_norm: 2.4602  loss: 0.8869\n",
            "06/07 17:30:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [3][50/96]  lr: 1.2646e-03  eta: 19:16:08  time: 2.4135  data_time: 0.0028  memory: 12844  grad_norm: 2.3371  loss: 0.9253\n",
            "06/07 17:31:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [3][55/96]  lr: 1.2906e-03  eta: 19:15:45  time: 2.4120  data_time: 0.0020  memory: 12844  grad_norm: 2.3440  loss: 0.9155\n",
            "06/07 17:31:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [3][60/96]  lr: 1.3167e-03  eta: 19:15:23  time: 2.4111  data_time: 0.0027  memory: 12844  grad_norm: 2.3925  loss: 0.9027\n",
            "06/07 17:31:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [3][65/96]  lr: 1.3427e-03  eta: 19:15:00  time: 2.4096  data_time: 0.0028  memory: 12844  grad_norm: 2.2917  loss: 0.8504\n",
            "06/07 17:31:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [3][70/96]  lr: 1.3687e-03  eta: 19:14:39  time: 2.4102  data_time: 0.0027  memory: 12844  grad_norm: 2.4423  loss: 0.8483\n",
            "06/07 17:31:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [3][75/96]  lr: 1.3948e-03  eta: 19:14:15  time: 2.4086  data_time: 0.0032  memory: 12844  grad_norm: 2.6255  loss: 0.9908\n",
            "06/07 17:32:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [3][80/96]  lr: 1.4208e-03  eta: 19:13:53  time: 2.4062  data_time: 0.0045  memory: 12844  grad_norm: 2.5347  loss: 1.0328\n",
            "06/07 17:32:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [3][85/96]  lr: 1.4468e-03  eta: 19:13:33  time: 2.4099  data_time: 0.0052  memory: 12844  grad_norm: 2.4895  loss: 0.9499\n",
            "06/07 17:32:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [3][90/96]  lr: 1.4728e-03  eta: 19:13:08  time: 2.4063  data_time: 0.0028  memory: 12844  grad_norm: 2.5031  loss: 0.9549\n",
            "06/07 17:32:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [3][95/96]  lr: 1.4989e-03  eta: 19:12:40  time: 2.3980  data_time: 0.0013  memory: 12844  grad_norm: 2.6291  loss: 1.1234\n",
            "06/07 17:32:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: convmixer-768-32_10xb64_in1k_20230607_171958\n",
            "06/07 17:32:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 3 epochs\n",
            "06/07 17:32:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [3][ 5/27]    eta: 0:00:20  time: 0.8150  data_time: 0.1073  memory: 4674  \n",
            "06/07 17:32:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [3][10/27]    eta: 0:00:13  time: 0.8196  data_time: 0.1075  memory: 558  \n",
            "06/07 17:32:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [3][15/27]    eta: 0:00:09  time: 0.7070  data_time: 0.0010  memory: 558  \n",
            "06/07 17:32:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [3][20/27]    eta: 0:00:05  time: 0.7073  data_time: 0.0011  memory: 558  \n",
            "06/07 17:33:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [3][25/27]    eta: 0:00:01  time: 0.7076  data_time: 0.0009  memory: 558  \n",
            "06/07 17:33:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][27/27]    accuracy/top1: 74.7100  data_time: 0.0390  time: 0.7438\n",
            "06/07 17:33:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [4][ 5/96]  lr: 1.5301e-03  eta: 19:11:54  time: 2.3812  data_time: 0.1263  memory: 12844  grad_norm: 2.8710  loss: 1.1876\n",
            "06/07 17:33:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [4][10/96]  lr: 1.5561e-03  eta: 19:11:36  time: 2.5422  data_time: 0.1263  memory: 12844  grad_norm: 2.4511  loss: 1.0083\n",
            "06/07 17:33:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [4][15/96]  lr: 1.5822e-03  eta: 19:11:19  time: 2.4139  data_time: 0.0038  memory: 12844  grad_norm: 2.6120  loss: 1.0305\n",
            "06/07 17:33:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [4][20/96]  lr: 1.6082e-03  eta: 19:11:04  time: 2.4160  data_time: 0.0054  memory: 12844  grad_norm: 2.5074  loss: 1.0106\n",
            "06/07 17:34:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [4][25/96]  lr: 1.6342e-03  eta: 19:10:40  time: 2.4079  data_time: 0.0032  memory: 12844  grad_norm: 2.2663  loss: 1.0620\n",
            "06/07 17:34:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [4][30/96]  lr: 1.6603e-03  eta: 19:10:16  time: 2.3973  data_time: 0.0027  memory: 12844  grad_norm: 2.3300  loss: 1.1161\n",
            "06/07 17:34:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [4][35/96]  lr: 1.6863e-03  eta: 19:09:50  time: 2.3937  data_time: 0.0026  memory: 12844  grad_norm: 2.4969  loss: 1.1016\n",
            "06/07 17:34:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [4][40/96]  lr: 1.7123e-03  eta: 19:09:29  time: 2.3975  data_time: 0.0035  memory: 12844  grad_norm: 2.6026  loss: 1.1307\n",
            "06/07 17:34:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [4][45/96]  lr: 1.7383e-03  eta: 19:09:06  time: 2.3992  data_time: 0.0034  memory: 12844  grad_norm: 2.4779  loss: 1.0895\n",
            "06/07 17:35:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [4][50/96]  lr: 1.7644e-03  eta: 19:08:44  time: 2.3970  data_time: 0.0021  memory: 12844  grad_norm: 2.2760  loss: 1.0172\n",
            "06/07 17:35:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [4][55/96]  lr: 1.7904e-03  eta: 19:08:19  time: 2.3956  data_time: 0.0021  memory: 12844  grad_norm: 2.3516  loss: 1.0999\n",
            "06/07 17:35:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [4][60/96]  lr: 1.8164e-03  eta: 19:07:59  time: 2.3965  data_time: 0.0029  memory: 12844  grad_norm: 2.4349  loss: 1.2062\n",
            "06/07 17:35:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [4][65/96]  lr: 1.8425e-03  eta: 19:07:41  time: 2.4033  data_time: 0.0044  memory: 12844  grad_norm: 2.3295  loss: 1.1483\n",
            "06/07 17:35:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [4][70/96]  lr: 1.8685e-03  eta: 19:07:21  time: 2.4041  data_time: 0.0041  memory: 12844  grad_norm: 2.4025  loss: 1.1357\n",
            "06/07 17:36:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [4][75/96]  lr: 1.8945e-03  eta: 19:06:59  time: 2.3977  data_time: 0.0029  memory: 12844  grad_norm: 2.4434  loss: 1.1366\n",
            "06/07 17:36:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [4][80/96]  lr: 1.9205e-03  eta: 19:06:43  time: 2.4017  data_time: 0.0027  memory: 12844  grad_norm: 2.3532  loss: 1.0668\n",
            "06/07 17:36:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [4][85/96]  lr: 1.9466e-03  eta: 19:06:27  time: 2.4100  data_time: 0.0034  memory: 12844  grad_norm: 2.4335  loss: 1.1159\n",
            "06/07 17:36:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [4][90/96]  lr: 1.9726e-03  eta: 19:06:09  time: 2.4068  data_time: 0.0024  memory: 12844  grad_norm: 2.4548  loss: 1.1746\n",
            "06/07 17:36:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [4][95/96]  lr: 1.9986e-03  eta: 19:05:48  time: 2.3998  data_time: 0.0012  memory: 12844  grad_norm: 2.3279  loss: 1.0919\n",
            "06/07 17:36:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: convmixer-768-32_10xb64_in1k_20230607_171958\n",
            "06/07 17:36:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 4 epochs\n",
            "06/07 17:36:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [4][ 5/27]    eta: 0:00:21  time: 0.8378  data_time: 0.1284  memory: 4674  \n",
            "06/07 17:37:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [4][10/27]    eta: 0:00:14  time: 0.8418  data_time: 0.1285  memory: 558  \n",
            "06/07 17:37:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [4][15/27]    eta: 0:00:09  time: 0.7070  data_time: 0.0009  memory: 558  \n",
            "06/07 17:37:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [4][20/27]    eta: 0:00:05  time: 0.7076  data_time: 0.0008  memory: 558  \n",
            "06/07 17:37:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [4][25/27]    eta: 0:00:01  time: 0.7071  data_time: 0.0008  memory: 558  \n",
            "06/07 17:37:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][27/27]    accuracy/top1: 70.7657  data_time: 0.0464  time: 0.7518\n",
            "06/07 17:37:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [5][ 5/96]  lr: 2.0299e-03  eta: 19:05:08  time: 2.3750  data_time: 0.1267  memory: 12844  grad_norm: 2.4115  loss: 1.0495\n",
            "06/07 17:37:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [5][10/96]  lr: 2.0559e-03  eta: 19:04:54  time: 2.5374  data_time: 0.1268  memory: 12844  grad_norm: 2.2823  loss: 1.1273\n",
            "06/07 17:37:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [5][15/96]  lr: 2.0819e-03  eta: 19:04:40  time: 2.4130  data_time: 0.0023  memory: 12844  grad_norm: 2.3772  loss: 1.2209\n",
            "06/07 17:38:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [5][20/96]  lr: 2.1080e-03  eta: 19:04:23  time: 2.4094  data_time: 0.0036  memory: 12844  grad_norm: 2.4151  loss: 1.3948\n",
            "06/07 17:38:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [5][25/96]  lr: 2.1340e-03  eta: 19:04:06  time: 2.4038  data_time: 0.0037  memory: 12844  grad_norm: 2.2131  loss: 1.3639\n",
            "06/07 17:38:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [5][30/96]  lr: 2.1600e-03  eta: 19:03:43  time: 2.3939  data_time: 0.0025  memory: 12844  grad_norm: 2.0362  loss: 1.2174\n",
            "06/07 17:38:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [5][35/96]  lr: 2.1860e-03  eta: 19:03:25  time: 2.3944  data_time: 0.0040  memory: 12844  grad_norm: 2.2566  loss: 1.4110\n",
            "06/07 17:38:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [5][40/96]  lr: 2.2121e-03  eta: 19:03:08  time: 2.4021  data_time: 0.0054  memory: 12844  grad_norm: 2.3102  loss: 1.5002\n",
            "06/07 17:39:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [5][45/96]  lr: 2.2381e-03  eta: 19:02:50  time: 2.3998  data_time: 0.0030  memory: 12844  grad_norm: 2.2026  loss: 1.3619\n",
            "06/07 17:39:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [5][50/96]  lr: 2.2641e-03  eta: 19:02:29  time: 2.3941  data_time: 0.0023  memory: 12844  grad_norm: 2.1998  loss: 1.2158\n",
            "06/07 17:39:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [5][55/96]  lr: 2.2902e-03  eta: 19:02:10  time: 2.3928  data_time: 0.0040  memory: 12844  grad_norm: 2.3124  loss: 1.3004\n",
            "06/07 17:39:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [5][60/96]  lr: 2.3162e-03  eta: 19:01:53  time: 2.3982  data_time: 0.0046  memory: 12844  grad_norm: 2.2584  loss: 1.4103\n",
            "06/07 17:39:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [5][65/96]  lr: 2.3422e-03  eta: 19:01:36  time: 2.4013  data_time: 0.0042  memory: 12844  grad_norm: 2.0364  loss: 1.4033\n",
            "06/07 17:40:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [5][70/96]  lr: 2.3682e-03  eta: 19:01:16  time: 2.3959  data_time: 0.0029  memory: 12844  grad_norm: 1.9458  loss: 1.3078\n",
            "06/07 17:40:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [5][75/96]  lr: 2.3943e-03  eta: 19:00:59  time: 2.3946  data_time: 0.0022  memory: 12844  grad_norm: 1.9185  loss: 1.1477\n",
            "06/07 17:40:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [5][80/96]  lr: 2.4203e-03  eta: 19:00:46  time: 2.4053  data_time: 0.0032  memory: 12844  grad_norm: 2.0282  loss: 1.1696\n",
            "06/07 17:40:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [5][85/96]  lr: 2.4463e-03  eta: 19:00:31  time: 2.4096  data_time: 0.0033  memory: 12844  grad_norm: 2.0457  loss: 1.1814\n",
            "06/07 17:40:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [5][90/96]  lr: 2.4724e-03  eta: 19:00:15  time: 2.4047  data_time: 0.0021  memory: 12844  grad_norm: 1.9755  loss: 1.1940\n",
            "06/07 17:41:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [5][95/96]  lr: 2.4984e-03  eta: 18:59:58  time: 2.4005  data_time: 0.0013  memory: 12844  grad_norm: 2.0279  loss: 1.2849\n",
            "06/07 17:41:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: convmixer-768-32_10xb64_in1k_20230607_171958\n",
            "06/07 17:41:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 5 epochs\n",
            "06/07 17:41:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [5][ 5/27]    eta: 0:00:22  time: 0.8576  data_time: 0.1519  memory: 4674  \n",
            "06/07 17:41:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [5][10/27]    eta: 0:00:14  time: 0.8634  data_time: 0.1527  memory: 558  \n",
            "06/07 17:41:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [5][15/27]    eta: 0:00:09  time: 0.7080  data_time: 0.0019  memory: 558  \n",
            "06/07 17:41:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [5][20/27]    eta: 0:00:05  time: 0.7079  data_time: 0.0010  memory: 558  \n",
            "06/07 17:41:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [5][25/27]    eta: 0:00:01  time: 0.7085  data_time: 0.0007  memory: 558  \n",
            "06/07 17:41:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][27/27]    accuracy/top1: 73.0858  data_time: 0.0551  time: 0.7601\n",
            "06/07 17:41:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [6][ 5/96]  lr: 2.5296e-03  eta: 18:59:34  time: 2.3889  data_time: 0.1436  memory: 12844  grad_norm: 2.2645  loss: 1.2868\n",
            "06/07 17:41:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [6][10/96]  lr: 2.5557e-03  eta: 18:59:20  time: 2.5509  data_time: 0.1444  memory: 12844  grad_norm: 2.0635  loss: 1.1671\n",
            "06/07 17:42:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [6][15/96]  lr: 2.5817e-03  eta: 18:59:07  time: 2.4104  data_time: 0.0032  memory: 12844  grad_norm: 2.0884  loss: 1.2851\n",
            "06/07 17:42:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [6][20/96]  lr: 2.6077e-03  eta: 18:58:53  time: 2.4093  data_time: 0.0026  memory: 12844  grad_norm: 1.9618  loss: 1.2854\n",
            "06/07 17:42:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [6][25/96]  lr: 2.6337e-03  eta: 18:58:39  time: 2.4071  data_time: 0.0017  memory: 12844  grad_norm: 1.8863  loss: 1.1688\n",
            "06/07 17:42:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [6][30/96]  lr: 2.6598e-03  eta: 18:58:26  time: 2.4104  data_time: 0.0030  memory: 12844  grad_norm: 1.9726  loss: 1.1952\n",
            "06/07 17:42:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [6][35/96]  lr: 2.6858e-03  eta: 18:58:16  time: 2.4172  data_time: 0.0036  memory: 12844  grad_norm: 1.9413  loss: 1.1376\n",
            "06/07 17:43:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [6][40/96]  lr: 2.7118e-03  eta: 18:58:04  time: 2.4178  data_time: 0.0040  memory: 12844  grad_norm: 2.0517  loss: 1.2352\n",
            "06/07 17:43:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [6][45/96]  lr: 2.7379e-03  eta: 18:57:49  time: 2.4092  data_time: 0.0039  memory: 12844  grad_norm: 2.0641  loss: 1.3902\n",
            "06/07 17:43:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [6][50/96]  lr: 2.7639e-03  eta: 18:57:30  time: 2.3955  data_time: 0.0028  memory: 12844  grad_norm: 1.9537  loss: 1.3253\n",
            "06/07 17:43:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [6][55/96]  lr: 2.7899e-03  eta: 18:57:12  time: 2.3905  data_time: 0.0023  memory: 12844  grad_norm: 1.8966  loss: 1.2884\n",
            "06/07 17:43:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [6][60/96]  lr: 2.8159e-03  eta: 18:56:54  time: 2.3917  data_time: 0.0023  memory: 12844  grad_norm: 1.9245  loss: 1.3315\n",
            "06/07 17:44:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [6][65/96]  lr: 2.8420e-03  eta: 18:56:36  time: 2.3910  data_time: 0.0028  memory: 12844  grad_norm: 1.9009  loss: 1.4256\n",
            "06/07 17:44:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [6][70/96]  lr: 2.8680e-03  eta: 18:56:17  time: 2.3894  data_time: 0.0032  memory: 12844  grad_norm: 1.8241  loss: 1.4700\n",
            "06/07 17:44:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [6][75/96]  lr: 2.8940e-03  eta: 18:56:04  time: 2.3984  data_time: 0.0050  memory: 12844  grad_norm: 1.9210  loss: 1.3605\n",
            "06/07 17:44:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [6][80/96]  lr: 2.9201e-03  eta: 18:55:48  time: 2.4037  data_time: 0.0039  memory: 12844  grad_norm: 2.0446  loss: 1.2727\n",
            "06/07 17:44:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [6][85/96]  lr: 2.9461e-03  eta: 18:55:35  time: 2.4035  data_time: 0.0032  memory: 12844  grad_norm: 2.0389  loss: 1.2993\n",
            "06/07 17:45:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [6][90/96]  lr: 2.9721e-03  eta: 18:55:18  time: 2.4018  data_time: 0.0031  memory: 12844  grad_norm: 2.0368  loss: 1.4981\n",
            "06/07 17:45:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [6][95/96]  lr: 2.9982e-03  eta: 18:55:02  time: 2.3956  data_time: 0.0013  memory: 12844  grad_norm: 1.8599  loss: 1.5373\n",
            "06/07 17:45:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: convmixer-768-32_10xb64_in1k_20230607_171958\n",
            "06/07 17:45:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 6 epochs\n",
            "06/07 17:45:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [6][ 5/27]    eta: 0:00:22  time: 0.8543  data_time: 0.1458  memory: 4674  \n",
            "06/07 17:45:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [6][10/27]    eta: 0:00:14  time: 0.8580  data_time: 0.1470  memory: 558  \n",
            "06/07 17:45:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [6][15/27]    eta: 0:00:09  time: 0.7074  data_time: 0.0021  memory: 558  \n",
            "06/07 17:45:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [6][20/27]    eta: 0:00:05  time: 0.7075  data_time: 0.0009  memory: 558  \n",
            "06/07 17:45:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [6][25/27]    eta: 0:00:01  time: 0.7081  data_time: 0.0007  memory: 558  \n",
            "06/07 17:45:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [6][27/27]    accuracy/top1: 68.0975  data_time: 0.0530  time: 0.7579\n",
            "06/07 17:45:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [7][ 5/96]  lr: 3.0294e-03  eta: 18:54:53  time: 2.4136  data_time: 0.1704  memory: 12844  grad_norm: 1.6722  loss: 1.3264\n",
            "06/07 17:46:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [7][10/96]  lr: 3.0554e-03  eta: 18:54:42  time: 2.5830  data_time: 0.1719  memory: 12844  grad_norm: 1.5818  loss: 1.1691\n",
            "06/07 17:46:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [7][15/96]  lr: 3.0814e-03  eta: 18:54:30  time: 2.4152  data_time: 0.0037  memory: 12844  grad_norm: 1.8148  loss: 1.2214\n",
            "06/07 17:46:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [7][20/96]  lr: 3.1075e-03  eta: 18:54:15  time: 2.4061  data_time: 0.0032  memory: 12844  grad_norm: 1.9045  loss: 1.2551\n",
            "06/07 17:46:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [7][25/96]  lr: 3.1335e-03  eta: 18:53:58  time: 2.3980  data_time: 0.0032  memory: 12844  grad_norm: 1.9246  loss: 1.2866\n",
            "06/07 17:46:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [7][30/96]  lr: 3.1595e-03  eta: 18:53:42  time: 2.3948  data_time: 0.0043  memory: 12844  grad_norm: 1.9257  loss: 1.3222\n",
            "06/07 17:47:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [7][35/96]  lr: 3.1856e-03  eta: 18:53:26  time: 2.3942  data_time: 0.0042  memory: 12844  grad_norm: 1.8630  loss: 1.3250\n",
            "06/07 17:47:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [7][40/96]  lr: 3.2116e-03  eta: 18:53:12  time: 2.3990  data_time: 0.0036  memory: 12844  grad_norm: 1.7652  loss: 1.2980\n",
            "06/07 17:47:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [7][45/96]  lr: 3.2376e-03  eta: 18:52:53  time: 2.3938  data_time: 0.0031  memory: 12844  grad_norm: 1.7714  loss: 1.3939\n",
            "06/07 17:47:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [7][50/96]  lr: 3.2636e-03  eta: 18:52:36  time: 2.3864  data_time: 0.0016  memory: 12844  grad_norm: 1.7426  loss: 1.3139\n",
            "06/07 17:47:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [7][55/96]  lr: 3.2897e-03  eta: 18:52:21  time: 2.3944  data_time: 0.0032  memory: 12844  grad_norm: 1.7327  loss: 1.1801\n",
            "06/07 17:48:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [7][60/96]  lr: 3.3157e-03  eta: 18:52:05  time: 2.3964  data_time: 0.0045  memory: 12844  grad_norm: 1.8474  loss: 1.2805\n",
            "06/07 17:48:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [7][65/96]  lr: 3.3417e-03  eta: 18:51:47  time: 2.3903  data_time: 0.0029  memory: 12844  grad_norm: 1.8346  loss: 1.3411\n",
            "06/07 17:48:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [7][70/96]  lr: 3.3678e-03  eta: 18:51:32  time: 2.3912  data_time: 0.0034  memory: 12844  grad_norm: 1.8231  loss: 1.3995\n",
            "06/07 17:48:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [7][75/96]  lr: 3.3938e-03  eta: 18:51:17  time: 2.3971  data_time: 0.0046  memory: 12844  grad_norm: 1.8564  loss: 1.4263\n",
            "06/07 17:48:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [7][80/96]  lr: 3.4198e-03  eta: 18:51:01  time: 2.3972  data_time: 0.0035  memory: 12844  grad_norm: 1.7709  loss: 1.4626\n",
            "06/07 17:49:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [7][85/96]  lr: 3.4459e-03  eta: 18:50:47  time: 2.3980  data_time: 0.0028  memory: 12844  grad_norm: 1.6510  loss: 1.4369\n",
            "06/07 17:49:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [7][90/96]  lr: 3.4719e-03  eta: 18:50:32  time: 2.3997  data_time: 0.0028  memory: 12844  grad_norm: 1.6553  loss: 1.3841\n",
            "06/07 17:49:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [7][95/96]  lr: 3.4979e-03  eta: 18:50:19  time: 2.4021  data_time: 0.0020  memory: 12844  grad_norm: 1.7335  loss: 1.4258\n",
            "06/07 17:49:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: convmixer-768-32_10xb64_in1k_20230607_171958\n",
            "06/07 17:49:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 7 epochs\n",
            "06/07 17:49:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [7][ 5/27]    eta: 0:00:18  time: 0.7686  data_time: 0.0637  memory: 4674  \n",
            "06/07 17:49:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [7][10/27]    eta: 0:00:13  time: 0.7770  data_time: 0.0650  memory: 558  \n",
            "06/07 17:49:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [7][15/27]    eta: 0:00:09  time: 0.7128  data_time: 0.0021  memory: 558  \n",
            "06/07 17:49:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [7][20/27]    eta: 0:00:05  time: 0.7093  data_time: 0.0008  memory: 558  \n",
            "06/07 17:49:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)   [7][25/27]    eta: 0:00:01  time: 0.7091  data_time: 0.0006  memory: 558  \n",
            "06/07 17:49:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [7][27/27]    accuracy/top1: 64.3851  data_time: 0.0237  time: 0.7298\n",
            "06/07 17:50:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [8][ 5/96]  lr: 3.5291e-03  eta: 18:50:30  time: 2.4697  data_time: 0.2133  memory: 12844  grad_norm: 1.9665  loss: 1.4319\n",
            "06/07 17:50:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [8][10/96]  lr: 3.5552e-03  eta: 18:50:21  time: 2.6358  data_time: 0.2149  memory: 12844  grad_norm: 1.8102  loss: 1.4383\n",
            "06/07 17:50:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [8][15/96]  lr: 3.5812e-03  eta: 18:50:09  time: 2.4199  data_time: 0.0037  memory: 12844  grad_norm: 1.6676  loss: 1.4061\n",
            "06/07 17:50:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [8][20/96]  lr: 3.6072e-03  eta: 18:49:56  time: 2.4113  data_time: 0.0030  memory: 12844  grad_norm: 1.7485  loss: 1.4894\n",
            "06/07 17:50:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [8][25/96]  lr: 3.6333e-03  eta: 18:49:42  time: 2.4042  data_time: 0.0043  memory: 12844  grad_norm: 1.7746  loss: 1.5071\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/mmpretrain/tools/train.py\", line 159, in <module>\n",
            "    main()\n",
            "Exception in thread Thread-2 (_pin_memory_loop):\n",
            "  File \"/content/mmpretrain/tools/train.py\", line 155, in main\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    runner.train()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mmengine/runner/runner.py\", line 1721, in train\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    model = self.train_loop.run()  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mmengine/runner/loops.py\", line 96, in run\n",
            "    self.run_epoch()\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mmengine/runner/loops.py\", line 112, in run_epoch\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py\", line 51, in _pin_memory_loop\n",
            "    self.run_iter(idx, data_batch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mmengine/runner/loops.py\", line 128, in run_iter\n",
            "    do_one_step()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py\", line 28, in do_one_step\n",
            "    outputs = self.runner.model.train_step(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mmengine/model/base_model/base_model.py\", line 116, in train_step\n",
            "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n",
            "    optim_wrapper.update_params(parsed_losses)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mmengine/optim/optimizer/optimizer_wrapper.py\", line 189, in update_params\n",
            "    return _ForkingPickler.loads(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/reductions.py\", line 307, in rebuild_storage_fd\n",
            "    self.step(**step_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mmengine/optim/scheduler/param_scheduler.py\", line 115, in wrapper\n",
            "    fd = df.detach()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/resource_sharer.py\", line 57, in detach\n",
            "    return wrapped(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mmengine/optim/optimizer/optimizer_wrapper.py\", line 240, in step\n",
            "    with _resource_sharer.get_connection(self._id) as conn:\n",
            "  File \"/usr/lib/python3.10/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
            "    self._clip_grad()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mmengine/optim/optimizer/optimizer_wrapper.py\", line 362, in _clip_grad\n",
            "    c = Client(address, authkey=process.current_process().authkey)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 502, in Client\n",
            "    grad = self.clip_func(params, **self.clip_grad_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/utils/clip_grad.py\", line 76, in clip_grad_norm_\n",
            "    c = SocketClient(address)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 630, in SocketClient\n",
            "    torch._foreach_mul_(grads, clip_coef_clamped.to(device))  # type: ignore[call-overload]\n",
            "KeyboardInterrupt\n",
            "    s.connect(address)\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#推理\n",
        "!python /content/mmpretrain/tools/test.py /content/mmpretrain/work_dirs/convmixer-768-32_10xb64_in1k/convmixer-768-32_10xb64_in1k.py /content/mmpretrain/work_dirs/convmixer-768-32_10xb64_in1k/epoch_5.pth\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZ-4Y_qvsB_z",
        "outputId": "d65324dc-4a20-4de8-81c7-296405035cb4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "06/07 16:32:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
            "------------------------------------------------------------\n",
            "System environment:\n",
            "    sys.platform: linux\n",
            "    Python: 3.10.11 (main, Apr  5 2023, 14:15:10) [GCC 9.4.0]\n",
            "    CUDA available: True\n",
            "    numpy_random_seed: 1207970244\n",
            "    GPU 0: Tesla T4\n",
            "    CUDA_HOME: /usr/local/cuda\n",
            "    NVCC: Cuda compilation tools, release 11.8, V11.8.89\n",
            "    GCC: x86_64-linux-gnu-gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "    PyTorch: 2.0.1+cu118\n",
            "    PyTorch compiling details: PyTorch built with:\n",
            "  - GCC 9.3\n",
            "  - C++ Version: 201703\n",
            "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 11.8\n",
            "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
            "  - CuDNN 8.7\n",
            "  - Magma 2.6.1\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
            "\n",
            "    TorchVision: 0.15.2+cu118\n",
            "    OpenCV: 4.7.0\n",
            "    MMEngine: 0.7.4\n",
            "\n",
            "Runtime environment:\n",
            "    cudnn_benchmark: False\n",
            "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
            "    dist_cfg: {'backend': 'nccl'}\n",
            "    seed: 1207970244\n",
            "    deterministic: False\n",
            "    Distributed launcher: none\n",
            "    Distributed training: False\n",
            "    GPU number: 1\n",
            "------------------------------------------------------------\n",
            "\n",
            "06/07 16:32:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Config:\n",
            "model = dict(\n",
            "    type='ImageClassifier',\n",
            "    backbone=dict(type='ConvMixer', arch='768/32', act_cfg=dict(type='ReLU')),\n",
            "    neck=dict(type='GlobalAveragePooling'),\n",
            "    head=dict(\n",
            "        type='LinearClsHead',\n",
            "        num_classes=30,\n",
            "        in_channels=768,\n",
            "        loss=dict(type='CrossEntropyLoss', loss_weight=1.0)))\n",
            "dataset_type = 'CustomDataset'\n",
            "data_preprocessor = dict(\n",
            "    num_classes=30,\n",
            "    mean=[123.675, 116.28, 103.53],\n",
            "    std=[58.395, 57.12, 57.375],\n",
            "    to_rgb=True)\n",
            "bgr_mean = [103.53, 116.28, 123.675]\n",
            "bgr_std = [57.375, 57.12, 58.395]\n",
            "train_pipeline = [\n",
            "    dict(type='LoadImageFromFile'),\n",
            "    dict(\n",
            "        type='RandomResizedCrop',\n",
            "        scale=224,\n",
            "        backend='pillow',\n",
            "        interpolation='bicubic'),\n",
            "    dict(type='RandomFlip', prob=0.5, direction='horizontal'),\n",
            "    dict(\n",
            "        type='RandAugment',\n",
            "        policies='timm_increasing',\n",
            "        num_policies=2,\n",
            "        total_level=10,\n",
            "        magnitude_level=9,\n",
            "        magnitude_std=0.5,\n",
            "        hparams=dict(pad_val=[104, 116, 124], interpolation='bicubic')),\n",
            "    dict(\n",
            "        type='RandomErasing',\n",
            "        erase_prob=0.25,\n",
            "        mode='rand',\n",
            "        min_area_ratio=0.02,\n",
            "        max_area_ratio=0.3333333333333333,\n",
            "        fill_color=[103.53, 116.28, 123.675],\n",
            "        fill_std=[57.375, 57.12, 58.395]),\n",
            "    dict(type='PackInputs')\n",
            "]\n",
            "test_pipeline = [\n",
            "    dict(type='LoadImageFromFile'),\n",
            "    dict(\n",
            "        type='ResizeEdge',\n",
            "        scale=233,\n",
            "        edge='short',\n",
            "        backend='pillow',\n",
            "        interpolation='bicubic'),\n",
            "    dict(type='CenterCrop', crop_size=224),\n",
            "    dict(type='PackInputs')\n",
            "]\n",
            "train_dataloader = dict(\n",
            "    pin_memory=True,\n",
            "    persistent_workers=True,\n",
            "    collate_fn=dict(type='default_collate'),\n",
            "    batch_size=32,\n",
            "    num_workers=5,\n",
            "    dataset=dict(\n",
            "        type='CustomDataset',\n",
            "        data_root='/content/mmpretrain/data/train',\n",
            "        pipeline=[\n",
            "            dict(type='LoadImageFromFile'),\n",
            "            dict(\n",
            "                type='RandomResizedCrop',\n",
            "                scale=224,\n",
            "                backend='pillow',\n",
            "                interpolation='bicubic'),\n",
            "            dict(type='RandomFlip', prob=0.5, direction='horizontal'),\n",
            "            dict(\n",
            "                type='RandAugment',\n",
            "                policies='timm_increasing',\n",
            "                num_policies=2,\n",
            "                total_level=10,\n",
            "                magnitude_level=9,\n",
            "                magnitude_std=0.5,\n",
            "                hparams=dict(pad_val=[104, 116, 124],\n",
            "                             interpolation='bicubic')),\n",
            "            dict(\n",
            "                type='RandomErasing',\n",
            "                erase_prob=0.25,\n",
            "                mode='rand',\n",
            "                min_area_ratio=0.02,\n",
            "                max_area_ratio=0.3333333333333333,\n",
            "                fill_color=[103.53, 116.28, 123.675],\n",
            "                fill_std=[57.375, 57.12, 58.395]),\n",
            "            dict(type='PackInputs')\n",
            "        ]),\n",
            "    sampler=dict(type='DefaultSampler', shuffle=True))\n",
            "val_dataloader = dict(\n",
            "    pin_memory=True,\n",
            "    persistent_workers=True,\n",
            "    collate_fn=dict(type='default_collate'),\n",
            "    batch_size=32,\n",
            "    num_workers=5,\n",
            "    dataset=dict(\n",
            "        type='CustomDataset',\n",
            "        data_root='/content/mmpretrain/data/valid',\n",
            "        pipeline=[\n",
            "            dict(type='LoadImageFromFile'),\n",
            "            dict(\n",
            "                type='ResizeEdge',\n",
            "                scale=233,\n",
            "                edge='short',\n",
            "                backend='pillow',\n",
            "                interpolation='bicubic'),\n",
            "            dict(type='CenterCrop', crop_size=224),\n",
            "            dict(type='PackInputs')\n",
            "        ]),\n",
            "    sampler=dict(type='DefaultSampler', shuffle=False))\n",
            "val_evaluator = dict(type='Accuracy', topk=(1, 5))\n",
            "test_dataloader = dict(\n",
            "    pin_memory=True,\n",
            "    collate_fn=dict(type='default_collate'),\n",
            "    persistent_workers=True,\n",
            "    batch_size=32,\n",
            "    num_workers=5,\n",
            "    dataset=dict(\n",
            "        type='CustomDataset',\n",
            "        data_root='/content/mmpretrain/data/test',\n",
            "        pipeline=[\n",
            "            dict(type='LoadImageFromFile'),\n",
            "            dict(\n",
            "                type='ResizeEdge',\n",
            "                scale=233,\n",
            "                edge='short',\n",
            "                backend='pillow',\n",
            "                interpolation='bicubic'),\n",
            "            dict(type='CenterCrop', crop_size=224),\n",
            "            dict(type='PackInputs')\n",
            "        ]),\n",
            "    sampler=dict(type='DefaultSampler', shuffle=False))\n",
            "test_evaluator = dict(type='Accuracy', topk=(1, 5))\n",
            "optim_wrapper = dict(\n",
            "    optimizer=dict(\n",
            "        type='AdamW',\n",
            "        lr=0.01,\n",
            "        weight_decay=0.05,\n",
            "        eps=1e-08,\n",
            "        betas=(0.9, 0.999)),\n",
            "    paramwise_cfg=dict(\n",
            "        norm_decay_mult=0.0,\n",
            "        bias_decay_mult=0.0,\n",
            "        flat_decay_mult=0.0,\n",
            "        custom_keys=dict({\n",
            "            '.absolute_pos_embed': dict(decay_mult=0.0),\n",
            "            '.relative_position_bias_table': dict(decay_mult=0.0)\n",
            "        })),\n",
            "    clip_grad=dict(max_norm=5.0))\n",
            "param_scheduler = [\n",
            "    dict(\n",
            "        type='LinearLR',\n",
            "        start_factor=0.001,\n",
            "        by_epoch=True,\n",
            "        end=10,\n",
            "        convert_to_iter_based=True),\n",
            "    dict(type='CosineAnnealingLR', eta_min=1e-05, by_epoch=True, begin=20)\n",
            "]\n",
            "train_cfg = dict(by_epoch=True, max_epochs=300, val_interval=1)\n",
            "val_cfg = dict()\n",
            "test_cfg = dict()\n",
            "auto_scale_lr = dict(base_batch_size=640)\n",
            "default_scope = 'mmpretrain'\n",
            "default_hooks = dict(\n",
            "    timer=dict(type='IterTimerHook'),\n",
            "    logger=dict(type='LoggerHook', interval=5),\n",
            "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
            "    checkpoint=dict(type='CheckpointHook', interval=1),\n",
            "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
            "    visualization=dict(type='VisualizationHook', enable=False))\n",
            "env_cfg = dict(\n",
            "    cudnn_benchmark=False,\n",
            "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),\n",
            "    dist_cfg=dict(backend='nccl'))\n",
            "vis_backends = [dict(type='LocalVisBackend')]\n",
            "visualizer = dict(\n",
            "    type='UniversalVisualizer', vis_backends=[dict(type='LocalVisBackend')])\n",
            "log_level = 'INFO'\n",
            "load_from = '/content/mmpretrain/work_dirs/convmixer-768-32_10xb64_in1k/epoch_5.pth'\n",
            "resume = False\n",
            "randomness = dict(seed=None, deterministic=False)\n",
            "launcher = 'none'\n",
            "work_dir = './work_dirs/convmixer-768-32_10xb64_in1k'\n",
            "\n",
            "06/07 16:32:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
            "06/07 16:32:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
            "before_run:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "before_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_train_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) DistSamplerSeedHook                \n",
            " -------------------- \n",
            "before_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_train_iter:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_train_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_val_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "before_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) VisualizationHook                  \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_val_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(LOW         ) ParamSchedulerHook                 \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_train:\n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_test_epoch:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "before_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) VisualizationHook                  \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_run:\n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Loads checkpoint by local backend from path: /content/mmpretrain/work_dirs/convmixer-768-32_10xb64_in1k/epoch_5.pth\n",
            "06/07 16:32:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Load checkpoint from /content/mmpretrain/work_dirs/convmixer-768-32_10xb64_in1k/epoch_5.pth\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "06/07 16:32:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(test) [ 5/15]    eta: 0:00:15  time: 1.5451  data_time: 0.2436  memory: 394  \n",
            "06/07 16:32:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(test) [10/15]    eta: 0:00:05  time: 1.0966  data_time: 0.1222  memory: 394  \n",
            "06/07 16:32:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(test) [15/15]    eta: 0:00:00  time: 0.6150  data_time: 0.0007  memory: 394  \n",
            "06/07 16:32:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(test) [15/15]    accuracy/top1: 3.6797  accuracy/top5: 19.4805  data_time: 0.0816  time: 0.9250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#测试效果\n",
        "!python /content/mmpretrain/demo/image_demo.py /content/mmpretrain/data/test/山竹/103.jpg \\\n",
        "/content/mmpretrain/work_dirs/convmixer-768-32_10xb64_in1k/convmixer-768-32_10xb64_in1k.py \\\n",
        "--checkpoint /content/mmpretrain/work_dirs/convmixer-768-32_10xb64_in1k/epoch_2.pth --show --show-dir ./\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gr3wgMKsSSF",
        "outputId": "cae8c9f6-02b4-4180-ea40-d8280c3e792c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loads checkpoint by local backend from path: /content/mmpretrain/work_dirs/convmixer-768-32_10xb64_in1k/epoch_2.pth\n",
            "\u001b[2KInference \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h06/07 17:51:33 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - `Visualizer` backend is not initialized because save_dir is None.\n",
            "/usr/local/lib/python3.10/dist-packages/mmengine/visualization/utils.py:240: UserWarning: Glyph 23665 (\\N{CJK UNIFIED IDEOGRAPH-5C71}) missing from current font.\n",
            "  s, (width, height) = canvas.print_to_buffer()\n",
            "/usr/local/lib/python3.10/dist-packages/mmengine/visualization/utils.py:240: UserWarning: Glyph 31481 (\\N{CJK UNIFIED IDEOGRAPH-7AF9}) missing from current font.\n",
            "  s, (width, height) = canvas.print_to_buffer()\n",
            "/usr/local/lib/python3.10/dist-packages/mmengine/visualization/utils.py:240: UserWarning: Glyph 23665 (\\N{CJK UNIFIED IDEOGRAPH-5C71}) missing from current font.\n",
            "  s, (width, height) = canvas.print_to_buffer()\n",
            "/usr/local/lib/python3.10/dist-packages/mmengine/visualization/utils.py:240: UserWarning: Glyph 31481 (\\N{CJK UNIFIED IDEOGRAPH-7AF9}) missing from current font.\n",
            "  s, (width, height) = canvas.print_to_buffer()\n",
            "\u001b[1m{\u001b[0m\n",
            "  \u001b[1;34m\"pred_label\"\u001b[0m: \u001b[1;36m2\u001b[0m,\n",
            "  \u001b[1;34m\"pred_score\"\u001b[0m: \u001b[1;36m0.964999794960022\u001b[0m,\n",
            "  \u001b[1;34m\"pred_class\"\u001b[0m: \u001b[32m\"山竹\"\u001b[0m\n",
            "\u001b[1m}\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}